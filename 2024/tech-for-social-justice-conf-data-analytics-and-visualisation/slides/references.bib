@article{Lum2016,
author = {Lum, Kristian and Isaac, William},
title = {To predict and serve?},
journal = {Significance},
volume = {13},
number = {5},
pages = {14-19},
doi = {https://doi.org/10.1111/j.1740-9713.2016.00960.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1740-9713.2016.00960.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1740-9713.2016.00960.x},
abstract = {Predictive policing systems are used increasingly by law enforcement to try to prevent crime before it occurs. But what happens when these systems are trained using biased data? Kristian Lum and William Isaac consider the evidence – and the social consequences},
year = {2016}
}
@misc{Devries2019,
      title={Does Object Recognition Work for Everyone?}, 
      author={Terrance DeVries and Ishan Misra and Changhan Wang and Laurens van der Maaten},
      year={2019},
      eprint={1906.02659},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{Gebru2021,
  author       = {Timnit Gebru and
                  Jamie Morgenstern and
                  Briana Vecchione and
                  Jennifer Wortman Vaughan and
                  Hanna M. Wallach and
                  Hal Daum{\'{e}} III and
                  Kate Crawford},
  title        = {Datasheets for Datasets},
  journal      = {CoRR},
  volume       = {abs/1803.09010},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.09010},
  eprinttype    = {arXiv},
  eprint       = {1803.09010},
  timestamp    = {Mon, 20 Aug 2018 15:16:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-09010.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{Lundberg2017,
  author       = {Scott M. Lundberg and
                  Su{-}In Lee},
  title        = {A unified approach to interpreting model predictions},
  journal      = {CoRR},
  volume       = {abs/1705.07874},
  year         = {2017},
  url          = {http://arxiv.org/abs/1705.07874},
  eprinttype    = {arXiv},
  eprint       = {1705.07874},
  timestamp    = {Fri, 26 Nov 2021 16:33:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/LundbergL17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{Ribeiro2016,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@inproceedings{Amini2019,
author = {Amini, Alexander and Soleimany, Ava P. and Schwarting, Wilko and Bhatia, Sangeeta N. and Rus, Daniela},
title = {Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure},
year = {2019},
isbn = {9781450363242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306618.3314243},
doi = {10.1145/3306618.3314243},
abstract = {Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.},
booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {289–295},
numpages = {7},
keywords = {deep learning, facial detection, neural networks, algorithmic bias},
location = {Honolulu, HI, USA},
series = {AIES '19}
}

@inproceedings{Shuang2020,
author = {Shuang, Kai and Xu, Meng and Zhang, Wentao and Zhang, Zhixuan},
title = {Adversarial Multi-Task Label Embedding for Text Classification},
year = {2020},
isbn = {9781450372596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372422.3372433},
doi = {10.1145/3372422.3372433},
abstract = {Multi-task learning makes use of the potential correlation among related tasks to perform well in text classification. However, in the most multi-task works, labels are converted to meaningless one-hot vectors, which cause the loss of label semantics closely related to text semantics. Besides, shared and private features captured by previous shared-private multi-task learning framework are usually confused by the fact that the shared unit simply shares the parameters. In this paper, we propose the Adversarial Multi-task Label Embedding model. In this model, we integrate label semantics and improve the performance of multi-task learning. We introduce adversarial training and orthogonal constraints into the multi-task learning framework to prevent shared features and private features from interacting with each other. Extensive experimental results on six benchmark datasets demonstrate that our proposed approach is superior to the state-of-the-art multi-task text classification methods.},
booktitle = {Proceedings of the 2019 2nd International Conference on Computational Intelligence and Intelligent Systems},
pages = {45–50},
numpages = {6},
keywords = {Label Embedding, Text Classification, Adversarial Training, Orthogonality Constraints, Multi-task learning},
location = {Bangkok, Thailand},
series = {CIIS '19}
}


@InProceedings{Buolamwini2018,
  title = 	 {Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification},
  author = 	 {Buolamwini, Joy and Gebru, Timnit},
  booktitle = 	 {Proceedings of the 1st Conference on Fairness, Accountability and Transparency},
  pages = 	 {77--91},
  year = 	 {2018},
  editor = 	 {Friedler, Sorelle A. and Wilson, Christo},
  volume = 	 {81},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--24 Feb},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf},
  url = 	 {https://proceedings.mlr.press/v81/buolamwini18a.html},
  abstract = 	 {Recent studies demonstrate that machine learning algorithms can discriminate based on classes like race and gender. In this work, we present an approach to evaluate bias present in automated facial analysis algorithms and datasets with respect to phenotypic subgroups. Using the dermatologist  approved Fitzpatrick Skin Type classification system, we characterize the gender and skin type distribution of two facial analysis benchmarks, IJB-A and Adience. We find that these datasets are overwhelmingly composed of lighter-skinned subjects (79.6% for IJB-A and 86.2% for Adience) and introduce a new facial analysis dataset which is balanced by gender and skin type. We evaluate 3 commercial gender classification systems using our dataset and show that darker-skinned females are the most misclassified group (with error rates of up to 34.7%). The maximum error rate for lighter-skinned males is 0.8%. The substantial disparities in the accuracy of classifying darker females, lighter females, darker males, and lighter males in gender classification systems require urgent attention if commercial companies are to build genuinely fair, transparent and accountable facial analysis algorithms.}
}
@inproceedings{Mitchell2019,
	doi = {10.1145/3287560.3287596},
  
	url = {https://doi.org/10.1145%2F3287560.3287596},
  
	year = 2019,
	month = {jan},
  
	publisher = {{ACM}
},
  
	author = {Margaret Mitchell and Simone Wu and Andrew Zaldivar and Parker Barnes and Lucy Vasserman and Ben Hutchinson and Elena Spitzer and Inioluwa Deborah Raji and Timnit Gebru},
  
	title = {Model Cards for Model Reporting},
  
	booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency}
}